{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_train: 23330\n",
      "Rows in df_test: 9999\n",
      "Features: 36\n"
     ]
    }
   ],
   "source": [
    "to_standardize = ['age', 'chloride_input_meq', 'fluid_net_input_ml', 'heartrate_max',\n",
    "                  'sysbp_min', 'diasbp_min', 'resprate_max', 'weight', 'mingcs', 'day_1_chl']\n",
    "\n",
    "df_train = pd.read_csv('../data_collection/train.csv')\n",
    "df_train = df_train.set_index(['subject_id', 'hadm_id', 'icustay_id'], verify_integrity=True)\n",
    "df_train = df_train.drop(columns=['sodium_max', 'bicarbonate_min'])\n",
    "df_train = df_train.rename(columns={'chloride_max': 'day_1_chl'})\n",
    "df_train = df_train.fillna(0)  # fill comorbidities\n",
    "df_test = pd.read_csv('../data_collection/test.csv')\n",
    "df_test = df_test.set_index(['subject_id', 'hadm_id', 'icustay_id'], verify_integrity=True)\n",
    "df_test = df_test.drop(columns=['sodium_max', 'bicarbonate_min'])\n",
    "df_test = df_test.rename(columns={'chloride_max': 'day_1_chl'})\n",
    "df_test = df_test.fillna(0)  # fill comorbidities\n",
    "print \"Rows in df_train: %s\" % len(df_train)\n",
    "print \"Rows in df_test: %s\" % len(df_test)\n",
    "\n",
    "scaler = RobustScaler().fit(df_train[to_standardize])\n",
    "df_train[to_standardize] = scaler.transform(df_train[to_standardize])\n",
    "df_test[to_standardize] = scaler.transform(df_test[to_standardize])\n",
    "\n",
    "if len(df_train.columns) != len(df_test.columns):\n",
    "    raise ValueError(\"Inconsistent feature columns\")\n",
    "print \"Features: %s\" % len(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes\n",
    "Use 2nd day outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_chloride: 35942\n"
     ]
    }
   ],
   "source": [
    "# Chloride data from day 2\n",
    "df = pd.read_csv('../data_collection/chloride.csv')\n",
    "df = df.query('icu_day == 2 & chloride_max.notnull()')\n",
    "df = df.set_index(['subject_id', 'hadm_id', 'icustay_id'], verify_integrity=True)\n",
    "df_chloride = df.filter(['chloride_max'])\n",
    "df_chloride['chl_110'] = (df_chloride['chloride_max'] >= 110).astype(int)\n",
    "print \"Rows in df_chloride: %s\" % len(df_chloride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows w/ no chloride data on day 2: 8459\n",
      "Testing rows w/ no chloride data on day 2: 3513\n",
      "Training set: 23330 rows (1362/5.84% hyperchloremic on day 2)\n",
      "Testing set :  9999 rows ( 630/6.30% hyperchloremic on day 2)\n",
      "Total set   : 33329 rows (1992/5.98% hyperchloremic on day 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.join(df_chloride, how='left')\n",
    "df_test = df_test.join(df_chloride, how='left')\n",
    "\n",
    "print \"Training rows w/ no chloride data on day 2: %s\" % df_train['chl_110'].isna().sum()\n",
    "print \"Testing rows w/ no chloride data on day 2: %s\" % df_test['chl_110'].isna().sum()\n",
    "df_train = df_train.fillna({'chl_110': 0})\n",
    "df_test = df_test.fillna({'chl_110': 0})\n",
    "\n",
    "print \"Training set: %5s rows (%4s/%.2f%% hyperchloremic on day 2)\" % \\\n",
    "    (len(df_train), int(sum(df_train['chl_110'])), 100*float(sum(df_train['chl_110']))/len(df_train))\n",
    "print \"Testing set : %5s rows (%4s/%.2f%% hyperchloremic on day 2)\" % \\\n",
    "    (len(df_test), int(sum(df_test['chl_110'])), 100*float(sum(df_test['chl_110']))/len(df_test))\n",
    "print \"Total set   : %5s rows (%4s/%.2f%% hyperchloremic on day 2)\" % \\\n",
    "    (len(df_train)+len(df_test), int(sum(df_train['chl_110'])+sum(df_test['chl_110'])),\n",
    "     100*float(sum(df_train['chl_110'])+sum(df_test['chl_110']))/(len(df_train)+len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_data.csv')\n",
    "df_test.to_csv('test_data.csv')\n",
    "\n",
    "with open('./error_analysis/scaler.pickle', 'wb') as f:\n",
    "    pickle.dump((to_standardize, scaler), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
